{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oyiglCi34Rd"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data import DataLoader\n",
        "import zipfile\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import cv2\n",
        "import pandas as pd\n",
        "import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation\n",
        "from keras.utils import to_categorical\n",
        "from keras import backend as k\n",
        "%matplotlib inline\n",
        "import pickle\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers import Dense, Input, Conv2D, Flatten, MaxPooling2D, Activation, Dropout, BatchNormalization\n",
        "from keras.layers import LeakyReLU\n",
        "from keras.optimizers import Adamax\n",
        "from keras.models import Model\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import backend as K\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "RESIZE_DIM=32\n",
        "FIG_WIDTH=20\n",
        "HEIGHT_PER_ROW=3\n"
      ],
      "metadata": {
        "id": "Jpm0Az1pG9OB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "\n",
        "zip_file_path = '/content/drive/MyDrive/dataset/handw_bangla.zip'\n",
        "output_dir = '/content/unzip'\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "# Extract the contents of the ZIP file\n",
        "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(output_dir)\n",
        "\n",
        "print(\"ZIP file extracted successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZRHzVXSa4IKm",
        "outputId": "75eefad2-2832-4a0b-baf8-3a787d4a8f2f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ZIP file extracted successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "data_dir=os.path.join('/content/unzip')\n",
        "paths_train_a=glob.glob(os.path.join(data_dir,'training-a','*.png'))\n",
        "paths_train_b=glob.glob(os.path.join(data_dir,'training-b','*.png'))\n",
        "paths_train_e=glob.glob(os.path.join(data_dir,'training-e','*.png'))\n",
        "paths_train_c=glob.glob(os.path.join(data_dir,'training-c','*.png'))\n",
        "paths_train_d=glob.glob(os.path.join(data_dir,'training-d','*.png'))\n",
        "paths_train_all=paths_train_a+paths_train_b+paths_train_c+paths_train_d+paths_train_e\n",
        "\n",
        "\n",
        "\n",
        "paths_test_a=glob.glob(os.path.join(data_dir,'testing-a','*.png'))\n",
        "paths_test_b=glob.glob(os.path.join(data_dir,'testing-b','*.png'))\n",
        "paths_test_e=glob.glob(os.path.join(data_dir,'testing-e','*.png'))\n",
        "paths_test_c=glob.glob(os.path.join(data_dir,'testing-c','*.png'))\n",
        "paths_test_d=glob.glob(os.path.join(data_dir,'testing-d','*.png'))\n",
        "paths_test_f=glob.glob(os.path.join(data_dir,'testing-f','*.png'))+glob.glob(os.path.join(data_dir,'testing-f','*.JPG'))\n",
        "paths_test_auga=glob.glob(os.path.join(data_dir,'testing-auga','*.png'))\n",
        "paths_test_augc=glob.glob(os.path.join(data_dir,'testing-augc','*.png'))\n",
        "paths_test_all=paths_test_a+paths_test_b+paths_test_c+paths_test_d+paths_test_e+paths_test_f+paths_test_auga+paths_test_augc\n",
        "\n",
        "path_label_train_a=os.path.join(data_dir,'training-a.csv')\n",
        "path_label_train_b=os.path.join(data_dir,'training-b.csv')\n",
        "path_label_train_e=os.path.join(data_dir,'training-e.csv')\n",
        "path_label_train_c=os.path.join(data_dir,'training-c.csv')\n",
        "path_label_train_d=os.path.join(data_dir,'training-d.csv')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ipy1ppp_4INn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_key(path):\n",
        "    # Separates the key of an image from the filepath\n",
        "    key = path.split(sep=os.sep)[-1]\n",
        "    return key\n",
        "\n",
        "def get_data(paths_img, path_label=None, resize_dim=None):\n",
        "    '''Reads images from the filepaths, resizes them (if given), and returns them in a numpy array.\n",
        "    Args:\n",
        "        paths_img: image filepaths\n",
        "        path_label: pass image label filepaths while processing training data, defaults to None while processing testing data\n",
        "        resize_dim: if given, the image is resized to resize_dim x resize_dim (optional)\n",
        "    Returns:\n",
        "        X: group of images\n",
        "        y: categorical true labels\n",
        "    '''\n",
        "    X = []  # Initialize empty list for resized images\n",
        "    for i, path in enumerate(paths_img):\n",
        "        img = cv2.imread(path, cv2.IMREAD_COLOR)  # Load images in color (BGR)\n",
        "\n",
        "        if resize_dim is not None:\n",
        "            img = cv2.resize(img, (resize_dim, resize_dim), interpolation=cv2.INTER_AREA)  # Resize image to resize_dim x resize_dim\n",
        "        X.append(img)  # Append image to the list\n",
        "\n",
        "        # Display progress\n",
        "        if i == len(paths_img) - 1:\n",
        "            end = '\\n'\n",
        "        else:\n",
        "            end = '\\r'\n",
        "        print('Processed {}/{}'.format(i + 1, len(paths_img)), end=end)\n",
        "\n",
        "    X = np.array(X)  # Transform list to numpy array\n",
        "    if path_label is None:\n",
        "        return X\n",
        "    else:\n",
        "        df = pd.read_csv(path_label)  # Read labels\n",
        "        df = df.set_index('filename')\n",
        "        y_label = [df.loc[get_key(path)]['digit'] for path in paths_img]  # Get the labels corresponding to the images\n",
        "        y = to_categorical(y_label, 10)  # Transform integer value to categorical variable\n",
        "        return X, y\n",
        "def imshow_group(X, y, y_pred=None, n_per_row=10, phase='processed'):\n",
        "    n_sample = len(X)\n",
        "    img_dim = X.shape[1]\n",
        "    j = np.ceil(n_sample / n_per_row)\n",
        "    fig = plt.figure(figsize=(FIG_WIDTH, HEIGHT_PER_ROW * j))\n",
        "    for i, img in enumerate(X):\n",
        "        plt.subplot(j, n_per_row, i + 1)\n",
        "        plt.imshow(img)\n",
        "        if phase == 'processed':\n",
        "            plt.title(np.argmax(y[i]))\n",
        "        if phase == 'prediction':\n",
        "            top_n = 3  # top 3 predictions with highest probabilities\n",
        "            ind_sorted = np.argsort(y_pred[i])[::-1]\n",
        "            h = img_dim + 4\n",
        "            for k in range(top_n):\n",
        "                string = 'pred: {} ({:.0f}%)\\n'.format(ind_sorted[k], y_pred[i, ind_sorted[k]] * 100)\n",
        "                plt.text(img_dim / 2, h, string, horizontalalignment='center', verticalalignment='center')\n",
        "                h += 4\n",
        "            if y is not None:\n",
        "                plt.text(img_dim / 2, -4, 'true label: {}'.format(np.argmax(y[i])),\n",
        "                         horizontalalignment='center', verticalalignment='center')\n",
        "        plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "def create_submission(predictions, keys, path):\n",
        "    result = pd.DataFrame(\n",
        "        predictions,\n",
        "        columns=['label'],\n",
        "        index=keys\n",
        "    )\n",
        "    result.index.name = 'key'\n",
        "    result.to_csv(path, index=True)\n",
        "# Rest of the code...\n"
      ],
      "metadata": {
        "id": "SRHQI-DI_50m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_a,y_train_a=get_data(paths_train_a,path_label_train_a,resize_dim=RESIZE_DIM)\n",
        "X_train_b,y_train_b=get_data(paths_train_b,path_label_train_b,resize_dim=RESIZE_DIM)\n",
        "X_train_c,y_train_c=get_data(paths_train_c,path_label_train_c,resize_dim=RESIZE_DIM)\n",
        "X_train_d,y_train_d=get_data(paths_train_d,path_label_train_d,resize_dim=RESIZE_DIM)\n",
        "X_train_e,y_train_e=get_data(paths_train_e,path_label_train_e,resize_dim=RESIZE_DIM)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fLFQdl4J_53d",
        "outputId": "c528be6c-6d9d-4360-82ca-d780c7964e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 19702/19702\n",
            "Processed 359/359\n",
            "Processed 24298/24298\n",
            "Processed 10908/10908\n",
            "Processed 16778/16778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all=np.concatenate((X_train_a,X_train_b,X_train_c,X_train_d,X_train_e),axis=0)\n",
        "y_train_all=np.concatenate((y_train_a,y_train_b,y_train_c,y_train_d,y_train_e),axis=0)\n",
        "X_train_all.shape, y_train_all.shape\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEkP008a_56l",
        "outputId": "36ba963c-bdc9-4297-a6bf-7e17e0ef7287"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((72045, 32, 32, 3), (72045, 10))"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plt.subplot(221)\n",
        "# plt.imshow(X_train_all[0], cmap=plt.get_cmap('gray'))\n",
        "# plt.subplot(222)\n",
        "# plt.imshow(X_train_all[1], cmap=plt.get_cmap('gray'))\n",
        "# plt.subplot(223)\n",
        "# plt.imshow(X_train_all[2], cmap=plt.get_cmap('gray'))\n",
        "# plt.subplot(224)\n",
        "# plt.imshow(X_train_all[3], cmap=plt.get_cmap('gray'))\n",
        "# # show the plot\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "gR6GHHAm_59e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# imgg=X_train_all[1]\n",
        "# hist = cv2.calcHist([imgg],[0],None,[256],[0,256])\n",
        "# plt.hist(imgg.ravel(),256,[0,256])\n",
        "\n",
        "# # show the plotting graph of an image\n",
        "\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "-YBI5dgt_6AN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_a=get_data(paths_test_a,resize_dim=RESIZE_DIM)\n",
        "X_test_b=get_data(paths_test_b,resize_dim=RESIZE_DIM)\n",
        "X_test_c=get_data(paths_test_c,resize_dim=RESIZE_DIM)\n",
        "X_test_d=get_data(paths_test_d,resize_dim=RESIZE_DIM)\n",
        "X_test_e=get_data(paths_test_e,resize_dim=RESIZE_DIM)\n",
        "X_test_f=get_data(paths_test_f,resize_dim=RESIZE_DIM)\n",
        "X_test_auga=get_data(paths_test_auga,resize_dim=RESIZE_DIM)\n",
        "X_test_augc=get_data(paths_test_augc,resize_dim=RESIZE_DIM)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "919s-WTa_6DF",
        "outputId": "25eb97a7-7d7b-415d-e015-c0e0a7da0ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processed 3489/3489\n",
            "Processed 69/69\n",
            "Processed 4381/4381\n",
            "Processed 1948/1948\n",
            "Processed 2970/2970\n",
            "Processed 495/495\n",
            "Processed 2168/2168\n",
            "Processed 2106/2106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_test_all=np.concatenate((X_test_a,X_test_b,X_test_c,X_test_d,X_test_e,X_test_f,X_test_auga,X_test_augc))"
      ],
      "metadata": {
        "id": "EUEd8N14_6F9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_tshow_all=X_test_all\n",
        "X_tshow_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLzJekk6_6JI",
        "outputId": "ec11527b-6dc4-4994-d452-7fe79e96be93"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(17626, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #X_train_all = X_train_all/255\n",
        "#X_test_all=X_test_all/255"
      ],
      "metadata": {
        "id": "pSoLtiNY_6MC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##X_train_all = X_train_all.reshape(X_train_all.shape[0],32, 32,1).astype('float32')\n",
        "#X_test_all = X_test_all.reshape(X_test_all.shape[0],32, 32,1).astype('float32')"
      ],
      "metadata": {
        "id": "QojuNfjl4XEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2hxLnhMT4XIE",
        "outputId": "b99c6484-c372-4def-ec2b-18c6bac351f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(72045, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# import torchvision.transforms as transforms\n",
        "# import torchvision.models as models\n",
        "# from torch.utils.data import Dataset, DataLoader"
      ],
      "metadata": {
        "id": "M5TN54Xl4XLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
        "\n",
        "# label_encoder = LabelEncoder()\n",
        "# y_train_all_encoded = label_encoder.fit_transform(np.argmax(y_train_all, axis=1))\n",
        "# y_train_all_reshaped = y_train_all_encoded.reshape(-1, 1)\n",
        "# onehot_encoder = OneHotEncoder(sparse=False)\n",
        "# y_train_all_onehot = onehot_encoder.fit_transform(y_train_all_reshaped)\n",
        "\n",
        "# print(y_train_all_encoded[:10])\n",
        "\n",
        "# y_train_all_onehot.shape"
      ],
      "metadata": {
        "id": "Td0voEXeZTBr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define the transformation\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.ToPILImage(),                    # Convert the array to PIL Image\n",
        "#     transforms.Resize((224, 224)),               # Resize the images to (224, 224)\n",
        "#     transforms.ToTensor(),                       # Convert the images to tensors\n",
        "#     transforms.Normalize(                        # Normalize the image tensors\n",
        "#         mean=[0.485, 0.456, 0.406],               # Mean values for normalization\n",
        "#         std=[0.229, 0.224, 0.225]                 # Standard deviation values for normalization\n",
        "#     )\n",
        "# ])"
      ],
      "metadata": {
        "id": "ymPOktUxjx32"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Define your custom dataset class\n",
        "# class CustomDataset(Dataset):\n",
        "#     def __init__(self, data, targets, transform=None):\n",
        "#         self.data = data\n",
        "#         self.targets = targets\n",
        "#         self.transform = transform\n",
        "\n",
        "#     # def __len__(self):\n",
        "#         return len(self.data)\n",
        "\n",
        "#     def __getitem__(self, index):\n",
        "#         x = self.data[index]\n",
        "#         y = self.targets[index]\n",
        "\n",
        "#         if self.transform:\n",
        "#             x = self.transform(x)\n",
        "\n",
        "#         return x, y\n",
        "\n",
        "\n",
        "# # Convert your NumPy arrays to PyTorch datasets\n",
        "# train_dataset = CustomDataset(X_train_all, y_train_all, transform=transform)\n",
        "# test_dataset = CustomDataset(X_test_all, None, transform=transform)  # No labels for testing dataset"
      ],
      "metadata": {
        "id": "znOt4ZbUjyF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n"
      ],
      "metadata": {
        "id": "5iGvaPjIAEu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "# Define your custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "c_6DHG4DvS02"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert your NumPy arrays to PyTorch datasets\n",
        "train_dataset = CustomDataset(X_train_all, y_train_all, transform=transform)\n",
        "test_dataset = CustomDataset(X_test_all, None, transform=transform)\n",
        "\n",
        "# Create data loaders\n",
        "batch_size = 32\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Use ResNet-15\n",
        "model_15 = models.resnet18(pretrained=True)\n",
        "num_classes = 10  # Number of output classes\n",
        "model_15.fc = nn.Linear(model_15.fc.in_features, num_classes)  # Replace the fully connected layer\n",
        "\n",
        "# Use ResNet-50\n",
        "model_50 = models.resnet50(pretrained=True)\n",
        "model_50.fc = nn.Linear(model_50.fc.in_features, num_classes)  # Replace the fully connected layer\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_15 = optim.Adam(model_15.parameters(), lr=0.001)\n",
        "optimizer_50 = optim.Adam(model_50.parameters(), lr=0.001)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z41UluchvS7C",
        "outputId": "9ece39a4-7009-42a5-dd48-641de0504388"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 78.4MB/s]\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n",
            "100%|██████████| 97.8M/97.8M [00:00<00:00, 168MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for ResNet-15\n",
        "num_epochs = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_15.to(device)\n",
        "for epoch in range(num_epochs):\n",
        "    model_15.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer_15.zero_grad()\n",
        "\n",
        "        outputs = model_15(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_15.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w2Szl_a6vS-N",
        "outputId": "3206d06c-6627-4c87-ad3d-aa1d5feb07f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.1147\n",
            "Epoch 2/10, Loss: 0.0578\n",
            "Epoch 3/10, Loss: 0.0458\n",
            "Epoch 4/10, Loss: 0.0381\n",
            "Epoch 5/10, Loss: 0.0310\n",
            "Epoch 6/10, Loss: 0.0268\n",
            "Epoch 7/10, Loss: 0.0218\n",
            "Epoch 8/10, Loss: 0.0190\n",
            "Epoch 9/10, Loss: 0.0152\n",
            "Epoch 10/10, Loss: 0.0135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save ResNet-15 model\n",
        "save_path_resnet15 = '/content/15.pth'\n",
        "torch.save(model_15.state_dict(), save_path_resnet15)\n"
      ],
      "metadata": {
        "id": "Cby17wswzf6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop for ResNet-50\n",
        "num_epochs = 10\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_50.to(device)\n",
        "for epoch in range(num_epochs):\n",
        "    model_50.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer_50.zero_grad()\n",
        "\n",
        "        outputs = model_50(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer_50.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f'Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}')\n",
        "    # Save ResNet-50 model\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "b5Eh1LTxvTEt",
        "outputId": "ee184761-6bec-4b61-f996-c26fa7b62908"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a66a84acf0e5>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Training loop for ResNet-50\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cuda'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'cpu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel_50\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "save_path_resnet50 = '/content/16.pth'\n",
        "torch.save(model_50.state_dict(), save_path_resnet50)\n"
      ],
      "metadata": {
        "id": "Pgr4ydsm1wCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "FDSUGtFbKx0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model_15.eval()\n",
        "model_50.eval()\n",
        "with torch.no_grad():\n",
        "    correct_15 = 0\n",
        "    correct_50 = 0\n",
        "    total = 0\n",
        "\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs_15 = model_15(images)\n",
        "        _, predicted_15 = torch.max(outputs_15, dim=1)\n",
        "        correct_15 += (predicted_15 == labels).sum().item()\n",
        "\n",
        "        outputs_50 = model_50(images)\n",
        "        _, predicted_50 = torch.max(outputs_50, dim=1)\n",
        "        correct_50 += (predicted_50 == labels).sum().item()\n",
        "\n",
        "        total += labels.size(0)\n",
        "\n",
        "    accuracy_15 = correct_15 / total\n",
        "    accuracy_50 = correct_50 / total\n",
        "\n",
        "    print(f'Accuracy of ResNet-15: {accuracy_15:.4f}')\n",
        "    print(f'Accuracy of ResNet-50: {accuracy_50:.4f}')"
      ],
      "metadata": {
        "id": "a3pDewn9vTII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1jfrBWQivTLh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 234
        },
        "id": "78B-I15QkNnh",
        "outputId": "3707320c-eef6-462d-9a18-8e7e29d1db07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-16d32209a31b>\u001b[0m in \u001b[0;36m<cell line: 14>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Move the model and data to the GPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;31m# Define the loss function and optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NSMWfYc5kNv7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lulBjUynkNy3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "haQbrH3AkN1v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JepTCrAZkN4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "# Define your custom dataset class\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index]\n",
        "        y = self.targets[index]\n",
        "\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "\n",
        "        return x, y\n",
        "\n",
        "# Convert your NumPy arrays to PyTorch datasets\n",
        "train_dataset = CustomDataset(X_train_all, y_train_all, transform=transform)\n"
      ],
      "metadata": {
        "id": "KoZheForY2_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Define the transformation\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),                      # Convert NumPy array to PIL Image\n",
        "    transforms.Resize((32, 32)),                   # Resize the images to the desired size\n",
        "    transforms.ToTensor()                          # Convert the images to tensors\n",
        "])\n",
        "\n",
        "# Apply the transformation to your dataset\n",
        "X_train_all_transformed = torch.stack([transform(img) for img in X_train_all])\n",
        "X_test_all_transformed = torch.stack([transform(img) for img in X_test_all])\n",
        "\n",
        "# Normalize the image tensors using NumPy operations\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std = np.array([0.229, 0.224, 0.225])\n",
        "X_train_all_normalized = (X_train_all_transformed - mean[:, None, None]) / std[:, None, None]\n",
        "X_test_all_normalized = (X_test_all_transformed - mean[:, None, None]) / std[:, None, None]\n"
      ],
      "metadata": {
        "id": "MVMP8x5pXjrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#alexnet = models.alexnet(pretrained=True)\n"
      ],
      "metadata": {
        "id": "DosZEMRz4XOG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet15 = models.resnet152(pretrained=True)\n"
      ],
      "metadata": {
        "id": "WSZMEuG84IQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "resnet50 = models.resnet50(pretrained=True)\n"
      ],
      "metadata": {
        "id": "nWmgmIDnRZBR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = 10  # Replace with the actual number of classes in your dataset\n",
        "\n",
        "#alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
        "resnet15.fc = nn.Linear(resnet15.fc.in_features, num_classes)\n",
        "resnet50.fc = nn.Linear(resnet50.fc.in_features, num_classes)\n"
      ],
      "metadata": {
        "id": "rW-ELOJ9RZEP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "#optimizer_alexnet = optim.SGD(alexnet.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_resnet15 = optim.SGD(resnet15.parameters(), lr=0.001, momentum=0.9)\n",
        "optimizer_resnet50 = optim.SGD(resnet50.parameters(), lr=0.001, momentum=0.9)\n"
      ],
      "metadata": {
        "id": "9JFrx64nRZHY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all_tensor = torch.from_numpy(X_train_all)\n",
        "y_train_all_tensor = torch.from_numpy(y_train_all)\n",
        "X_test_all_tensor = torch.from_numpy(X_test_all)\n"
      ],
      "metadata": {
        "id": "SrCdflCpRZKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = torch.utils.data.TensorDataset(X_train_all_tensor, y_train_all_tensor)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Assuming you have test labels, create a test dataset and data loader as well\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test_all_tensor)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n"
      ],
      "metadata": {
        "id": "c-Y1iuyfRZNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_all.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfInBYjof4Ga",
        "outputId": "596f3697-0ba8-427a-ba6d-82c39f4896f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(72045, 32, 32, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.ToPILImage(),\n",
        "    transforms.Resize((32, 32)),\n",
        "    transforms.Grayscale(num_output_channels=3),  # Convert to grayscale with 3 channels\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "        mean=[0.485, 0.456, 0.406],\n",
        "        std=[0.229, 0.224, 0.225]\n",
        "    )\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#alexnet.to(device)\n",
        "resnet15.to(device)\n",
        "resnet50.to(device)\n",
        "\n",
        "num_epochs = 10  # Adjust the number of epochs as needed\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Training loop\n",
        "    #alexnet.train()\n",
        "    resnet15.train()\n",
        "    resnet50.train()\n",
        "\n",
        "    for batch_idx, (data, target) in enumerate(train_loader):\n",
        "        data, target = data.to(device), target.to(device)\n",
        "\n",
        "        # Zero the gradients\n",
        "        #optimizer_alexnet.zero_grad()\n",
        "        optimizer_resnet15.zero_grad()\n",
        "        optimizer_resnet50.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        #output_alexnet = alexnet(data)\n",
        "        output_resnet15 = resnet15(data)\n",
        "        output_resnet50 = resnet50(data)\n",
        "\n",
        "        # Calculate loss\n",
        "        #loss_alexnet = criterion(output_alexnet, target)\n",
        "        loss_resnet15 = criterion(output_resnet15, target)\n",
        "        loss_resnet50 = criterion(output_resnet50, target)\n",
        "\n",
        "        # Backward pass\n",
        "        #loss_alexnet.backward()\n",
        "        loss_resnet15.backward()\n",
        "        loss_resnet50.backward()\n",
        "\n",
        "        # Update weights\n",
        "        #optimizer_alexnet.step()\n",
        "        optimizer_resnet15.step()\n",
        "        optimizer_resnet50.step()\n",
        "\n",
        "        # Print training progress\n",
        "        if (batch_idx+1) % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Batch [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    # Evaluation loop (optional, if you have test labels)\n",
        "    #alexnet.eval()\n",
        "    resnet15.eval()\n",
        "    resnet50.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        #correct_alexnet = 0\n",
        "        correct_resnet15 = 0\n",
        "        correct_resnet50 = 0\n",
        "        total = 0\n",
        "\n",
        "        for data in test_loader:\n",
        "            data = data.to(device)\n",
        "\n",
        "            #output_alexnet = alexnet(data)\n",
        "            output_resnet15 = resnet15(data)\n",
        "            output_resnet50 = resnet50(data)\n",
        "\n",
        "          #  _, predicted_alexnet = torch.max(output_alexnet.data, 1)\n",
        "            _, predicted_resnet15 = torch.max(output_resnet15.data, 1)\n",
        "            _, predicted_resnet50 = torch.max(output_resnet50.data, 1)\n",
        "\n",
        "            total += data.size(0)\n",
        "            #correct_alexnet += (predicted_alexnet == target).sum().item()\n",
        "            correct_resnet15 += (predicted_resnet15 == target).sum().item()\n",
        "            correct_resnet50 += (predicted_resnet50 == target).sum().item()\n",
        "\n",
        "        #accuracy_alexnet = 100 * correct_alexnet / total\n",
        "        accuracy_resnet15 = 100 * correct_resnet15 / total\n",
        "        accuracy_resnet50 = 100 * correct_resnet50 / total\n",
        "\n",
        "        #print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy (AlexNet): {accuracy_alexnet:.2f}%\")\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy (ResNet-15): {accuracy_resnet15:.2f}%\")\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Test Accuracy (ResNet-50): {accuracy_resnet50:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 414
        },
        "id": "OSDMZCGORr0g",
        "outputId": "944fc7c9-fbf0-4323-d274-cbea59b62f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-161-d1437cb736a6>\u001b[0m in \u001b[0;36m<cell line: 22>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;31m# Forward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;31m#output_alexnet = alexnet(data)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0moutput_resnet15\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet15\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moutput_resnet50\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresnet50\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/resnet.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    266\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    267\u001b[0m         \u001b[0;31m# See note [TorchScript super()]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 268\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    269\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    270\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [64, 3, 7, 7], expected input[64, 32, 32, 3] to have 3 channels, but got 32 channels instead"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KnoS72KSRr7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7jUvOX6JRr-G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6vJtSUcVRsA_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}